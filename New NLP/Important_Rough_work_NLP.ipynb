{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "reviews =[\"I love this series. I bought 1 and 2 on DVD\",\n",
        "         \"The Lion King 1 1/2 is a very cute story to go\",\n",
        "         \"Now this is the sort of film we used to get\"]"
      ],
      "metadata": {
        "id": "v8Cs5HsX1Awn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9kVoi365OfR",
        "outputId": "e05a5e63-9741-4815-add0-f85645f48bb5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I love this series. I bought 1 and 2 on DVD',\n",
              " 'The Lion King 1 1/2 is a very cute story to go',\n",
              " 'Now this is the sort of film we used to get']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data cleaning and preprocessing\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# For Stemming\n",
        "corpus = []\n",
        "for i in range(0, len(reviews)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', str(reviews[i]))\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "\n",
        "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h75NbzoD4mQf",
        "outputId": "adc48af2-1eae-4ef3-ef4e-9507eed8b92c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZQb7NdP5R5I",
        "outputId": "9f4a6693-445b-4389-d293-24d6dee98ae4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['love seri bought dvd', 'lion king cute stori go', 'sort film use get']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features=2500) #Max feature means top features in the corpus.\n",
        "X_bow = cv.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "id": "EyevEOyu3jQN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_bow # Why 13 values in each sentence because in BOW words the unique words after stopwords in placed at the column level as the corpus has 13 words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nluULW_e88eT",
        "outputId": "23f2b67e-ba15-43a9-ec54-9a3c4a9901e8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the TFIDF model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tv = TfidfVectorizer(max_features=2500)\n",
        "X_tfidf = tv.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "id": "cqCKZxni89_3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tfidf # Why 13 values in each sentence because in BOW words the unique words after stopwords in placed at the column level as the corpus has 13 words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9bqxsce9LeY",
        "outputId": "7043aa84-343d-498e-f5bf-b0c77628a68a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5      , 0.       , 0.5      , 0.       , 0.       , 0.       ,\n",
              "        0.       , 0.       , 0.5      , 0.5      , 0.       , 0.       ,\n",
              "        0.       ],\n",
              "       [0.       , 0.4472136, 0.       , 0.       , 0.       , 0.4472136,\n",
              "        0.4472136, 0.4472136, 0.       , 0.       , 0.       , 0.4472136,\n",
              "        0.       ],\n",
              "       [0.       , 0.       , 0.       , 0.5      , 0.5      , 0.       ,\n",
              "        0.       , 0.       , 0.       , 0.       , 0.5      , 0.       ,\n",
              "        0.5      ]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cretaing the Continious Bag of Words\n",
        "import gensim.downloader as api\n",
        "import gensim\n",
        "wv = api.load('word2vec-google-news-300') # google news data has 300 dimension\n",
        "model=gensim.models.Word2Vec(corpus,window=5,min_count=2,vector_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5CbUXUL9OCM",
        "outputId": "0816b8a6-ee0e-47d7-ccbc-4907d637fdce"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.corpus_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8f2QQbS-fow",
        "outputId": "4d94329e-2344-483b-ee41-ca40950ea8b2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert each document into a fixed-size vector\n",
        "X_cbow = [\n",
        "    sum(model.wv[word] for word in doc if word in model.wv) / len(doc) if len(doc) > 0 else [0.0] * 100\n",
        "    for doc in corpus\n",
        "]"
      ],
      "metadata": {
        "id": "kQ3RXJAI-_q7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_cbow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwNAwVBN_Cvy",
        "outputId": "134cfaa1-a605-4328-c565-a7f9b890e38e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-0.00294894,  0.00111289,  0.00503267,  0.00527408,  0.00091203,\n",
              "        -0.0025593 ,  0.01455606,  0.02898431, -0.02579695,  0.00405259],\n",
              "       dtype=float32),\n",
              " array([-0.01151155,  0.01597452,  0.02001062,  0.01496016, -0.00323439,\n",
              "        -0.02527533,  0.02799498,  0.01465593, -0.03285246, -0.01870072],\n",
              "       dtype=float32),\n",
              " array([-0.00673193,  0.02054774,  0.00682282,  0.00149946, -0.00512805,\n",
              "        -0.01443213,  0.01214427,  0.02612625, -0.02498194,  0.00209347],\n",
              "       dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Avgword2vec\n",
        "def avg_word2vec(doc):\n",
        "    # remove out-of-vocabulary words\n",
        "    #sent = [word for word in doc if word in model.wv.index_to_key]\n",
        "    #print(sent)\n",
        "\n",
        "    return np.mean([model.wv[word] for word in doc if word in model.wv.index_to_key],axis=0)\n",
        "                #or [np.zeros(len(model.wv.index_to_key))], axis=0)"
      ],
      "metadata": {
        "id": "opfDe-9f_dZ4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "#apply for the entire sentences\n",
        "X_avg=[]\n",
        "for i in tqdm(range(len(corpus))):\n",
        "    X_avg.append(avg_word2vec(corpus[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6F_KE_NBH1X5",
        "outputId": "e9bd0f90-f21f-46e2-d7e0-76288b8f3f5d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 1785.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_avg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnS9FDNTH469",
        "outputId": "219070ff-612e-4651-db0e-3520c14a2169"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-0.00327661,  0.00123655,  0.00559185,  0.00586009,  0.00101337,\n",
              "        -0.00284367,  0.0161734 ,  0.03220479, -0.02866328,  0.00450287],\n",
              "       dtype=float32),\n",
              " array([-0.01260788,  0.0174959 ,  0.0219164 ,  0.01638494, -0.00354243,\n",
              "        -0.02768251,  0.03066117,  0.01605173, -0.03598127, -0.02048174],\n",
              "       dtype=float32),\n",
              " array([-0.00762952,  0.02328744,  0.00773253,  0.00169938, -0.00581179,\n",
              "        -0.01635641,  0.0137635 ,  0.02960975, -0.02831287,  0.0023726 ],\n",
              "       dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zVpQuCBqH61Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}