{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8Cs5HsX1Awn"
   },
   "outputs": [],
   "source": [
    "reviews =[\"I love this series. I bought 1 and 2 on DVD\",\n",
    "         \"The Lion King 1 1/2 is a very cute story to go\",\n",
    "         \"Now this is the sort of film we used to get\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9kVoi365OfR",
    "outputId": "c4ff6e1f-e84e-4800-e587-56c194435597"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love this series. I bought 1 and 2 on DVD',\n",
       " 'The Lion King 1 1/2 is a very cute story to go',\n",
       " 'Now this is the sort of film we used to get']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h75NbzoD4mQf",
    "outputId": "6c98fcf4-7adf-4a12-96a3-b8671c55f5db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Data cleaning and preprocessing\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# For Stemming\n",
    "corpus = []\n",
    "for i in range(0, len(reviews)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', str(reviews[i]))\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "\n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZQb7NdP5R5I",
    "outputId": "bc3e2ede-323e-4b3c-83e9-035fe22972bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love seri bought dvd', 'lion king cute stori go', 'sort film use get']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyevEOyu3jQN"
   },
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=2500)\n",
    "X_bow = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nluULW_e88eT",
    "outputId": "be8d3e38-5126-459a-efce-c178d47bcee3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqCKZxni89_3"
   },
   "outputs": [],
   "source": [
    "# Creating the TFIDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features=2500)\n",
    "X_tfidf = tv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9bqxsce9LeY",
    "outputId": "2d6336c8-9dc2-44c9-996b-f7ce84cf041b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5      , 0.       , 0.5      , 0.       , 0.       , 0.       ,\n",
       "        0.       , 0.       , 0.5      , 0.5      , 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.4472136, 0.       , 0.       , 0.       , 0.4472136,\n",
       "        0.4472136, 0.4472136, 0.       , 0.       , 0.       , 0.4472136,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , 0.5      , 0.5      , 0.       ,\n",
       "        0.       , 0.       , 0.       , 0.       , 0.5      , 0.       ,\n",
       "        0.5      ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v5CbUXUL9OCM",
    "outputId": "1e0aae41-c7b0-4cdc-89c9-8ebdafa755f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
     ]
    }
   ],
   "source": [
    "# Cretaing the Continious Bag of Words\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "wv = api.load('word2vec-google-news-300') # google news data has 300 dimension\n",
    "model=gensim.models.Word2Vec(corpus,window=5,min_count=2,vector_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D8f2QQbS-fow",
    "outputId": "496fe2e3-0a31-420f-ba94-7ef2ce5a49eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQ3RXJAI-_q7"
   },
   "outputs": [],
   "source": [
    "# Convert each document into a fixed-size vector using the average of word vectors\n",
    "X_cbow = [\n",
    "    sum(model.wv[word] for word in doc if word in model.wv) / len(doc) if len(doc) > 0 else [0.0] * 100\n",
    "    for doc in corpus\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwNAwVBN_Cvy",
    "outputId": "134cfaa1-a605-4328-c565-a7f9b890e38e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.00294894,  0.00111289,  0.00503267,  0.00527408,  0.00091203,\n",
       "        -0.0025593 ,  0.01455606,  0.02898431, -0.02579695,  0.00405259],\n",
       "       dtype=float32),\n",
       " array([-0.01151155,  0.01597452,  0.02001062,  0.01496016, -0.00323439,\n",
       "        -0.02527533,  0.02799498,  0.01465593, -0.03285246, -0.01870072],\n",
       "       dtype=float32),\n",
       " array([-0.00673193,  0.02054774,  0.00682282,  0.00149946, -0.00512805,\n",
       "        -0.01443213,  0.01214427,  0.02612625, -0.02498194,  0.00209347],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opfDe-9f_dZ4"
   },
   "outputs": [],
   "source": [
    "# Creating the Avgword2vec\n",
    "def avg_word2vec(doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    #sent = [word for word in doc if word in model.wv.index_to_key]\n",
    "    #print(sent)\n",
    "\n",
    "    return np.mean([model.wv[word] for word in doc if word in model.wv.index_to_key],axis=0)\n",
    "                #or [np.zeros(len(model.wv.index_to_key))], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6F_KE_NBH1X5",
    "outputId": "156d6a45-a9b9-4777-9c55-89ba4366f60d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 592.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "#apply for the entire sentences\n",
    "X_avg=[]\n",
    "for i in tqdm(range(len(corpus))):\n",
    "    X_avg.append(avg_word2vec(corpus[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lnS9FDNTH469",
    "outputId": "c1f96cab-4a98-4989-a148-3053c0ba8418"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.00327661,  0.00123655,  0.00559185,  0.00586009,  0.00101337,\n",
       "        -0.00284367,  0.0161734 ,  0.03220479, -0.02866328,  0.00450287],\n",
       "       dtype=float32),\n",
       " array([-0.01260788,  0.0174959 ,  0.0219164 ,  0.01638494, -0.00354243,\n",
       "        -0.02768251,  0.03066117,  0.01605173, -0.03598127, -0.02048174],\n",
       "       dtype=float32),\n",
       " array([-0.00762952,  0.02328744,  0.00773253,  0.00169938, -0.00581179,\n",
       "        -0.01635641,  0.0137635 ,  0.02960975, -0.02831287,  0.0023726 ],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVpQuCBqH61Z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
